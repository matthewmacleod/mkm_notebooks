{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Tutorial\n",
    "\n",
    "Author: Matthew K. MacLeod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Goals\n",
    "\n",
    "* brief introduction\n",
    "* explore NLP from scratch \n",
    "* starting from simple approaches to text processing\n",
    "* contrast with more advanced approaches\n",
    "    * viterbi\n",
    "    * naive bayes\n",
    "    * n gram models\n",
    "    * statistical parsing\n",
    "    * tf-idf\n",
    "    * RNN\n",
    "    * LSTM\n",
    "\n",
    "\n",
    "## Procedure\n",
    "\n",
    "* download data\n",
    "* process data\n",
    "* NLP exploration\n",
    "\n",
    "Furthermore, use following tools as much as possible:\n",
    "\n",
    "* command line tools\n",
    "     * grep \n",
    "     * sed \n",
    "     * awk\n",
    "     * tr\n",
    "* regex\n",
    "* Python\n",
    "* our own code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First off let's go over some NLP linguistical jargon:\n",
    "\n",
    "### terminology:\n",
    "* **lemma**: same stem (**wordform** is the opposite of this)\n",
    "* **type**:  a basic element of the vocabulary\n",
    "* **vocabulary**: $|V|$, size of the type set\n",
    "* **tokens**: an instance of a type in the text, often denoted $N$\n",
    "* **morphemes**: smallest meaningful unit of word\n",
    "    * **stems**: core meaning part \n",
    "    * **affixes**: additional parts (to yeild grammatical agreement)\n",
    "* **stemming**: affix removal\n",
    "\n",
    "NB: in general\n",
    "    $|V| > \\sqrt{N}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Applications\n",
    "\n",
    "* simple applications\n",
    "    * spam detection\n",
    "    * part of speech (**POS**) eg noun, adj, etc\n",
    "    * named person recognition (**NER**) ie proper nouns\n",
    "    \n",
    "* medium difficulty applications\n",
    "    * sentiment analysis (quantify the polarity of a subject)\n",
    "    * parsing\n",
    "    * word sense disambiguation (**WSD**)\n",
    "    * information extraction (**IE**)\n",
    "    * machine translation (**MT**) between languages    \n",
    "\n",
    "* difficult tasks\n",
    "    * question answering\n",
    "    * paraphrasing\n",
    "    * summarization\n",
    "    * human-machine communication (dialog)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General NLP Challenges\n",
    "\n",
    "* ambiguity (multiple interpretations)\n",
    "    * **case folding** is important and can cause problems if not done carefully, eg:\n",
    "        * if one lowercases US to us now can be ambiguous\n",
    "        * Fed to fed\n",
    "* segmentation issues\n",
    "    * eg sentence segmentation based on period (.), use decision tree\n",
    "* idioms\n",
    "* neologisms\n",
    "* non-standard English (stuff found on the web)\n",
    "\n",
    "when working with **_actual text_** have the following **general** challenges\n",
    "\n",
    "* **type I errors**: false positives\n",
    "    * match things we shouldn't match\n",
    "    * eg when looking for **the**, also match o**the**r, O**the**llo, ect\n",
    "* **type II errors**: false negatives\n",
    "    * missing stuff we should have matched\n",
    "    * eg when search for **the**, miss capitalized **The**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique language challenges\n",
    "\n",
    "Each language comes with it's own set of peculiarities. An incomplete list of examples:\n",
    "\n",
    "English:\n",
    "* contraction expansion\n",
    "    \n",
    "French:\n",
    "* contraction expansion\n",
    "* gender\n",
    "\n",
    "German:\n",
    "* compound splitting of nouns (also Turkish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "\n",
    "Some sources from Peter Norvig and Beautiful data (some is already tokenized)\n",
    "\n",
    "http://norvig.com/ngrams/\n",
    "\n",
    "### Classical Literature \n",
    "\n",
    "#### Complete works:\n",
    "\n",
    "Mostly undefiled Shakespeare (we will need to remove some header info so we can mine it accurately):\n",
    "\n",
    "http://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\n",
    "    \n",
    "\n",
    "from Project Gutenberg (http://www.gutenberg.org/):\n",
    "   \n",
    "Austen, complete works:\n",
    "    \n",
    "http://www.gutenberg.org/cache/epub/31100/pg31100.txt\n",
    "    \n",
    "Shelley, complete works:\n",
    "\n",
    "http://www.gutenberg.org/cache/epub/4800/pg4800.txt\n",
    "    \n",
    "Aristotle, complete works:\n",
    "\n",
    "http://www.gutenberg.org/cache/epub/12699/pg12699.txt\n",
    "\n",
    "Plutarch:\n",
    "\n",
    "http://www.gutenberg.org/cache/epub/3052/pg3052.txt\n",
    "\n",
    "Walt Whitman, complete prose:\n",
    "\n",
    "http://www.gutenberg.org/cache/epub/8813/pg8813.txt\n",
    "\n",
    "Edgar Allan Poe, volumes:\n",
    "\n",
    "http://www.gutenberg.org/files/2147/2147-0.txt\n",
    "\n",
    "http://www.gutenberg.org/cache/epub/2148/pg2148.txt\n",
    "\n",
    "http://www.gutenberg.org/cache/epub/2149/pg2149.txt\n",
    "\n",
    "http://www.gutenberg.org/cache/epub/2150/pg2150.txt\n",
    "\n",
    "http://www.gutenberg.org/cache/epub/2151/pg2151.txt\n",
    "\n",
    "\n",
    "#### Selected works:\n",
    "\n",
    "http://www.gutenberg.org/cache/epub/12699/pg12699.txt\n",
    "\n",
    "Beyond Good and Evil, by Friedrich Nietzsche:\n",
    "\n",
    "http://www.gutenberg.org/cache/epub/4363/pg4363.txt\n",
    "\n",
    "in German:\n",
    "\n",
    "http://www.gutenberg.org/cache/epub/7204/pg7204.txt\n",
    "\n",
    "\n",
    "Zadig by Voltaire, in French:\n",
    "\n",
    "http://www.gutenberg.org/cache/epub/4647/pg4647.txt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP resources\n",
    "\n",
    "https://en.wikipedia.org/wiki/Natural_language_processing\n",
    "\n",
    "https://en.wikipedia.org/wiki/N-gram\n",
    "\n",
    "https://en.wikipedia.org/wiki/Katz's_back-off_model\n",
    "\n",
    "https://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation\n",
    "\n",
    "https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/matej/develop/mkm_notebooks/data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncomment to download \n",
    "#!wget http://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124456 t8.shakespeare.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Austen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!wget http://www.gutenberg.org/cache/epub/31100/pg31100.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80476 pg31100.txt\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l pg31100.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Others:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# other files Shelley\n",
    "#!wget http://www.gutenberg.org/cache/epub/4800/pg4800.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aristotle\n",
    "#!wget http://www.gutenberg.org/cache/epub/12699/pg12699.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Poe\n",
    "#!wget http://www.gutenberg.org/files/2147/2147-0.txt\n",
    "#!wget http://www.gutenberg.org/cache/epub/2148/pg2148.txt\n",
    "#!wget http://www.gutenberg.org/cache/epub/2149/pg2149.txt\n",
    "#!wget http://www.gutenberg.org/cache/epub/2150/pg2150.txt\n",
    "#!wget http://www.gutenberg.org/cache/epub/2151/pg2151.txt\n",
    "#!mv 2147-0.txt pg2147.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process files\n",
    "\n",
    "clean files, need to remove headers and footers so we are left with only the authors text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247:THE SONNETS\r\n"
     ]
    }
   ],
   "source": [
    "!grep -n SONNETS t8.shakespeare.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "THE SONNETS\r\n",
      "\r\n",
      "by William Shakespeare\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "                     1\r\n",
      "  From fairest creatures we desire increase,\r\n",
      "  That thereby beauty's rose might never die,\r\n",
      "  But as the riper should by time decease,\r\n",
      "  His tender heir might bear his memory:\r\n",
      "  But thou contracted to thine own bright eyes,\r\n",
      "  Feed'st thy light's flame with self-substantial fuel,\r\n",
      "  Making a famine where abundance lies,\r\n",
      "  Thy self thy foe, to thy sweet self too cruel:\r\n",
      "  Thou that art now the world's fresh ornament,\r\n",
      "  And only herald to the gaudy spring,\r\n",
      "  Within thine own bud buriest thy content,\r\n",
      "  And tender churl mak'st waste in niggarding:\r\n",
      "tail: error writing ‘standard output’: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "# get everything but the first 246 lines\n",
    "!tail -n +246 t8.shakespeare.txt | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the header edit \n",
    "!tail -n +246 t8.shakespeare.txt > temp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99210:THE END\r\n",
      "102255:THE END\r\n",
      "104915:THE END\r\n",
      "107783:THE END\r\n",
      "110792:THE END\r\n",
      "114791:THE END\r\n",
      "117608:THE END\r\n",
      "120181:THE END\r\n",
      "123792:THE END\r\n",
      "124193:THE END\r\n"
     ]
    }
   ],
   "source": [
    "# strip footer\n",
    "!grep -n \"THE END\" temp.txt | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "  'O, that infected moisture of his eye,\r\n",
      "  O, that false fire which in his cheek so glowed,\r\n",
      "  O, that forced thunder from his heart did fly,\r\n",
      "  O, that sad breath his spongy lungs bestowed,\r\n",
      "  O, all that borrowed motion, seeming owed,\r\n",
      "  Would yet again betray the fore-betrayed,\r\n",
      "  And new pervert a reconciled maid.'\r\n",
      "\r\n",
      "THE END\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 124193 temp.txt | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head -n 124193 temp.txt > shakespeare.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Austen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91:Chapter 1\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!grep -n Chapter pg31100.txt | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\r\n",
      "Chapter 1\r",
      "\r\n",
      "\r",
      "\r\n",
      "\r",
      "\r\n",
      "Sir Walter Elliot, of Kellynch Hall, in Somersetshire, was a man who,\r",
      "\r\n",
      "for his own amusement, never took up any book but the Baronetage; there\r",
      "\r\n",
      "he found occupation for an idle hour, and consolation in a distressed\r",
      "\r\n",
      "one; there his faculties were roused into admiration and respect, by\r",
      "\r\n",
      "contemplating the limited remnant of the earliest patents; there any\r",
      "\r\n",
      "unwelcome sensations, arising from domestic affairs changed naturally\r",
      "\r\n",
      "tail: error writing ‘standard output’: Broken pipe\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n +90 pg31100.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!tail -n +90 pg31100.txt > temp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80024:End of the Project Gutenberg EBook of The Complete Works of Jane Austen,\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!grep -n \"End of the Project Gutenberg\" temp.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n 80023 temp.txt > austen.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matej/develop/mkm_notebooks/data\n",
      "New lines:  59235 shelley.txt\n"
     ]
    }
   ],
   "source": [
    "# this is a bit tedious so write a script:\n",
    "!pwd\n",
    "!./clean_gutenberg.sh pg4800.txt shelley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lines:  11336 aristotle.txt\r\n"
     ]
    }
   ],
   "source": [
    "!./clean_gutenberg.sh pg12699.txt aristotle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New lines:  9195 poe1.txt\n",
      "New lines:  9833 poe2.txt\n",
      "New lines:  9605 poe3.txt\n",
      "New lines:  8505 poe4.txt\n",
      "New lines:  10282 poe5.txt\n",
      "47420 poe.txt\n"
     ]
    }
   ],
   "source": [
    "!./clean_gutenberg.sh pg2147.txt poe1\n",
    "!./clean_gutenberg.sh pg2148.txt poe2\n",
    "!./clean_gutenberg.sh pg2149.txt poe3\n",
    "!./clean_gutenberg.sh pg2150.txt poe4\n",
    "!./clean_gutenberg.sh pg2151.txt poe5\n",
    "!cp poe1.txt poe.txt; cat poe2.txt >> poe.txt; cat poe3.txt >> poe.txt; cat poe4.txt >>poe.txt; cat poe5.txt >>poe.txt \n",
    "!wc -l poe.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "### Simple analysis\n",
    "\n",
    "let's see some vocabulary sizes..this is an extremely simple word count analysis but is part of more complicated approaches. (Starting with Uni-gram models, bag of words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Bard's bag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "THE\r\n",
      "SONNETS\r\n",
      "by\r\n",
      "William\r\n",
      "Shakespeare\r\n",
      "From\r\n",
      "fairest\r\n",
      "creatures\r\n",
      "we\r\n",
      "tr: write error: Broken pipe\r\n",
      "tr: write error\r\n"
     ]
    }
   ],
   "source": [
    "# map nonwords to newlines\n",
    "!tr -sc 'A-Za-z' '\\n' < shakespeare.txt | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1 \n",
      "  12780 a\n",
      "   1945 A\n",
      "     25 Aaron\n",
      "     72 AARON\n",
      "      1 abaissiez\n",
      "     10 abandon\n",
      "      2 abandoned\n",
      "      2 abase\n",
      "      1 abash\n",
      "uniq: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# sort on alphabetical order:\n",
    "!tr -sc 'A-Za-z' '\\n' < shakespeare.txt | sort | uniq -c | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  23243 the\r\n",
      "  22225 I\r\n",
      "  18618 and\r\n",
      "  16339 to\r\n",
      "  15687 of\r\n",
      "  12780 a\r\n",
      "  12163 you\r\n",
      "  10839 my\r\n",
      "  10005 in\r\n",
      "   8954 d\r\n",
      "   8394 is\r\n",
      "   8333 that\r\n",
      "   8035 not\r\n",
      "   7752 me\r\n",
      "   7486 s\r\n",
      "   7457 And\r\n",
      "   6798 with\r\n",
      "   6750 it\r\n",
      "   6401 be\r\n",
      "   6322 his\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "# sort the lexicon on freq\n",
    "!tr -sc 'A-Za-z' '\\n' < shakespeare.txt | sort | uniq -c | sort -n -r | head -n 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  27595 the\r\n",
      "  26735 and\r\n",
      "  22538 i\r\n",
      "  19771 to\r\n",
      "  18132 of\r\n",
      "  14725 a\r\n",
      "  13826 you\r\n",
      "  12489 my\r\n",
      "  11535 that\r\n",
      "  11112 in\r\n",
      "   9753 is\r\n",
      "   8960 d\r\n",
      "   8729 not\r\n",
      "   8306 for\r\n",
      "   8008 with\r\n",
      "   7777 me\r\n",
      "   7725 it\r\n",
      "   7721 s\r\n",
      "   7114 be\r\n",
      "   6874 your\r\n",
      "sort: write failed: standard output: Broken pipe\r\n",
      "sort: write error\r\n"
     ]
    }
   ],
   "source": [
    "# map case to lower, sort on freq\n",
    "!tr -sc 'A-Za-z' '\\n' < shakespeare.txt | tr 'A-Z' 'a-z' |  sort | uniq -c | sort -n -r | head -n 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shakespeare's vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23529\r\n"
     ]
    }
   ],
   "source": [
    "# Shakespeare's vocabulary size:\n",
    "!tr -sc 'A-Za-z' '\\n' < shakespeare.txt | tr 'A-Z' 'a-z' |  sort | uniq -c | sort -n -r | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussion \n",
    "\n",
    "This is quite large! He is famous for creating new words. Actually, there are many that we still use today, eg _all_ words starting with the prefix '**un**', eg **un**just.\n",
    "\n",
    "https://en.wikipedia.org/wiki/William_Shakespeare\n",
    "\n",
    "http://www.shakespeareswords.com/FEW\n",
    "\n",
    "skepticism:\n",
    "\n",
    "https://www.bostonglobe.com/ideas/2013/08/17/coined-shakespeare-think-again/tWFE6b8qTD5gnybL5fOn8H/story.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some Other Vocabulary sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14638\r\n"
     ]
    }
   ],
   "source": [
    "# Austen\n",
    "!tr -sc 'A-Za-z' '\\n' < austen.txt | tr 'A-Z' 'a-z' |  sort | uniq -c | sort -n -r | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19527\r\n"
     ]
    }
   ],
   "source": [
    "# Shelley\n",
    "!tr -sc 'A-Za-z' '\\n' < shelley.txt | tr 'A-Z' 'a-z' |  sort | uniq -c | sort -n -r | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7240\r\n"
     ]
    }
   ],
   "source": [
    "# Aristotle\n",
    "!tr -sc 'A-Za-z' '\\n' < aristotle.txt | tr 'A-Z' 'a-z' |  sort | uniq -c | sort -n -r | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22192\r\n"
     ]
    }
   ],
   "source": [
    "# Poe\n",
    "!tr -sc 'A-Za-z' '\\n' < poe.txt | tr 'A-Z' 'a-z' |  sort | uniq -c | sort -n -r | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python version\n",
    "\n",
    "Now let's do the same with python, first simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thy', 'self', 'thy', 'foe', 'to', 'thy', 'sweet', 'self', 'too', 'cruel']\n"
     ]
    }
   ],
   "source": [
    "text=\"Thy self thy foe to thy sweet self too cruel\"\n",
    "words=text.split(\" \")\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'too', 'Thy', 'cruel', 'thy', 'foe', 'self', 'sweet']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniq_words = list(set(words))\n",
    "uniq_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniq_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probalistic Models\n",
    "\n",
    "The simplest of these models predict the probability of a word, phrase, or sentence with basic probabilty. For example, the probabilty of a sentence (S) given the following words is given by the chain rule:\n",
    "\n",
    "$P(S) = P(w_1, w_2,...w_n) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)...P(w_n|w_1...w_{n-1})= \\prod_i P(w_i|w_1w_2...w_{i-1})$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule for Shakespeare\n",
    "\n",
    "For example, from the segment\n",
    "\n",
    "\"Thy self thy foe to thy sweet self too cruel\"\n",
    " \n",
    "The probablity of the first four words is\n",
    "\n",
    "$P(\\mathrm{Thy\\ self\\ thy\\ foe}) = P(\\mathrm{Thy}) P(\\mathrm{self}|\\mathrm{Thy}) P(\\mathrm{thy}|\\mathrm{Thy\\ self}) P(\\mathrm{foe}|\\mathrm{Thy\\ self\\ thy})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next word probability\n",
    "\n",
    "The Shannon game of predicting the next word ($w_n$) is simply the conditional probability:\n",
    "\n",
    "$P(w_n|w_1,...w_{n-1})$\n",
    "\n",
    "ie what the probablity of $w_n$ given the previous sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplifying assumptions:\n",
    "\n",
    " * Markov Assumption\n",
    "     https://en.wikipedia.org/wiki/Markov_property\n",
    "     \n",
    " * Naive Bayes\n",
    " https://en.wikipedia.org/wiki/Naive_Bayes_classifier\n",
    "\n",
    "\n",
    "The **Markov approximation** states that the conditional probability distribution of future and past events depend only on the present state and not the sequence of events that created the present state. In this context it can be used to help predict next words by simply considering the last words:\n",
    "\n",
    "$P(\\mathrm{to} | \\mathrm{Thy\\ self\\ thy\\ foe}) \\approx P( \\mathrm{to} | \\mathrm{foe})$\n",
    "\n",
    "or two:\n",
    "\n",
    "$P(\\mathrm{to} | \\mathrm{Thy\\ self\\ thy\\ foe}) \\approx P( \\mathrm{to} | \\mathrm{thy\\ foe})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def n_gram(n, words):\n",
    "    ngrams = [words[i:i+n] for i in range(len(words)-(n-1))]\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def skip_bigram(k, n, words):\n",
    "    sgrams = [words[i] + \" \" + words[i+1+k] for i in range(len(words)-n)]\n",
    "    return sgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def skip_gram(k, n, words):\n",
    "    \"\"\" need to add a nested list comp to make this general\"\"\"\n",
    "    sgrams = [list([words[i]]) + [words[i+k+j] for j in range(1,n+k,k+1)] for i in range(len(words)-((1+k)*(n-1)))]\n",
    "    return sgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the in',\n",
       " 'rain Spain',\n",
       " 'in falls',\n",
       " 'Spain mainly',\n",
       " 'falls on',\n",
       " 'mainly the',\n",
       " 'on plain']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stext =\"the rain in Spain falls mainly on the plain\"\n",
    "swords = stext.split(\" \")\n",
    "skip_bigram(1,2,swords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'in'],\n",
       " ['rain', 'Spain'],\n",
       " ['in', 'falls'],\n",
       " ['Spain', 'mainly'],\n",
       " ['falls', 'on'],\n",
       " ['mainly', 'the'],\n",
       " ['on', 'plain']]"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram(1,2,swords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'in', 'falls'],\n",
       " ['rain', 'Spain', 'mainly'],\n",
       " ['in', 'falls', 'on'],\n",
       " ['Spain', 'mainly', 'the'],\n",
       " ['falls', 'on', 'plain']]"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram(1,3,swords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'Spain'],\n",
       " ['rain', 'falls'],\n",
       " ['in', 'mainly'],\n",
       " ['Spain', 'on'],\n",
       " ['falls', 'the'],\n",
       " ['mainly', 'plain']]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram(2,2,swords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'Spain', 'on'], ['rain', 'falls', 'the'], ['in', 'mainly', 'plain']]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"the rain in Spain falls mainly on the plain\"\n",
    "skip_gram(2,3,swords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Thy', 'self'],\n",
       " ['self', 'thy'],\n",
       " ['thy', 'foe'],\n",
       " ['foe', 'to'],\n",
       " ['to', 'thy'],\n",
       " ['thy', 'sweet'],\n",
       " ['sweet', 'self'],\n",
       " ['self', 'too'],\n",
       " ['too', 'cruel']]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = n_gram(2,words)\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thy self',\n",
       " 'self thy',\n",
       " 'thy foe',\n",
       " 'foe to',\n",
       " 'to thy',\n",
       " 'thy sweet',\n",
       " 'sweet self',\n",
       " 'self too',\n",
       " 'too cruel']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join(i) for i in bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Thy', 'self', 'thy'],\n",
       " ['self', 'thy', 'foe'],\n",
       " ['thy', 'foe', 'to'],\n",
       " ['foe', 'to', 'thy'],\n",
       " ['to', 'thy', 'sweet'],\n",
       " ['thy', 'sweet', 'self'],\n",
       " ['sweet', 'self', 'too'],\n",
       " ['self', 'too', 'cruel']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams= n_gram(3, words)\n",
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thy self thy',\n",
       " 'self thy foe',\n",
       " 'thy foe to',\n",
       " 'foe to thy',\n",
       " 'to thy sweet',\n",
       " 'thy sweet self',\n",
       " 'sweet self too',\n",
       " 'self too cruel']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join(i) for i in trigrams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note see file mkm_notebooks/license.txt for lgpl license of this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
