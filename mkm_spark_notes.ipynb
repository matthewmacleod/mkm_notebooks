{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Tutorial\n",
    "\n",
    "Author: Matthew K. MacLeod\n",
    "\n",
    "\n",
    "### Tutorial goals:\n",
    "\n",
    "* background\n",
    "* configuration\n",
    "* introduction\n",
    "* HQL\n",
    "* machine learning\n",
    "* streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Spark is an open source project created in 2009 by the UC Berkeley RAD lab.\n",
    "\n",
    "Spark is a distributed framework which uses the MapReduce paradigm in memory and much more.. \n",
    "\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.html\n",
    "\n",
    "Allows for\n",
    "* interactive queries (Spark SQL and Hive)\n",
    "* stream processing\n",
    "* data analytics (MLlib)\n",
    "* graph processing (GraphX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directed Acyclic Graph Scheduler\n",
    "\n",
    "Spark uses a DAG scheduler in order figure out how to execute the data analysis pipeline.\n",
    "\n",
    "DAGs help to make a dependency flow for the transformations..allows \n",
    "lineage and for recovery of lost data partitions.\n",
    "\n",
    "RDDs (spark objects, more on these below) are nodes and directed edges are are the transformations to create a execution graph.\n",
    "\n",
    "This DAG scheduler also allows for parallelization optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "Another important aspect to understanding Spark is caching. Since in- memory operations set Spark apart and is important to understand.\n",
    "\n",
    "in general \n",
    "    \n",
    "   * 10x to 100x speed ups typical\n",
    "    \n",
    "   * caching is gradual\n",
    "    \n",
    "   * fault tolerant (like rest of Spark)\n",
    "    \n",
    "   * want to cache the cleaned data set into memory\n",
    "    \n",
    "   * heavy calculations might want to use both memory and disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "download and install spark,\n",
    "    \n",
    "    cd spark-1.5.2\n",
    "    \n",
    "    ./sbt/sbt -Phive assembly\n",
    "\n",
    "set environmental variables:\n",
    "\n",
    "    export SPARK_HOME=\"$HOME/programs/spark/spark-1.5.2\"\n",
    "    \n",
    "    export PYSPARK_SUBMIT_ARGS=\"--master local[4] pyspark-shell\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matej/programs/spark/spark-1.5.2\r\n"
     ]
    }
   ],
   "source": [
    "# double check env \n",
    "!echo $SPARK_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will load configuration in notebook..this way don't need to configure profile (ipython has issues)\n",
    "\n",
    "    ipython notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.4.3 (default, Oct 19 2015 21:52:17)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "# spark configuration\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "\n",
    "filename = os.path.join(spark_home, 'python/pyspark/shell.py')\n",
    "exec(compile(open(filename, \"rb\").read(), filename, 'exec'))\n",
    "\n",
    "spark_release_file = spark_home + \"/RELEASE\"\n",
    "\n",
    "if os.path.exists(spark_release_file) and \"Spark 1.5\" in open(spark_release_file).read():\n",
    "    pyspark_submit_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"\")\n",
    "    if not \"pyspark-shell\" in pyspark_submit_args: \n",
    "        pyspark_submit_args += \" pyspark-shell\"\n",
    "        os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small note on setup,\n",
    "\n",
    "it may have been quicker to simply use:\n",
    "\n",
    "    IPTYON_OPTS=\"notebook\" ./bin/pyspark\n",
    "    \n",
    "to start the jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark\n",
    "\n",
    "  * starting spark, intialization\n",
    "  * RDD transformations\n",
    "  * RDD actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will run spark interactively in jupyter, \n",
    "\n",
    "normally a pyspark script can be run\n",
    "\n",
    "    bin/spark-submit ps.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guide,', 1), ('APIs', 1), ('optimized', 1), ('name', 1), ('Scala,', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the standard spark 'hello world' example:\n",
    "spark_home = os.environ.get('SPARK_HOME')\n",
    "\n",
    "text_file = sc.textFile(spark_home + \"/README.md\")\n",
    "\n",
    "word_counts = text_file \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "word_counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**collect** Return a list that contains all of the elements in this RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RDDs\n",
    "\n",
    "An RDD (_resilient distributed dataset_) is an **immutable** distributed data container... like a collection of objects. These have distributed partitions across nodes and clusters.\n",
    "\n",
    "Resilency is realized by tracking partitions and re-running partition history.\n",
    "\n",
    "RDDs can be created from many sources, including local text, Amazon S3, JSON, HDFS, HBase, Cassandra and other sources.\n",
    "\n",
    "RDD operations consist of transformations and actions.\n",
    "\n",
    "**transformations** in spark-speak, operate on RDDs and *return new RDDs*. These are evaluated **lazily**.  eg map() and filter() are transformations.\n",
    "\n",
    "Due to the immutablity of the RDDs, functional style of transforming the data is essential.\n",
    "\n",
    "\n",
    "### one pair RDD  Transformations\n",
    "\n",
    "* map\n",
    "* filter\n",
    "* reduceByKey\n",
    "* groupByKey\n",
    "* combineByKey\n",
    "* mapValues\n",
    "* flatMapValues\n",
    "* keys\n",
    "* values\n",
    "* sortByKey\n",
    "* sample(withReplacement, fraction ,seed)\n",
    "* coalesce(numPartitions)    to reduce number of partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locality\n",
    "\n",
    "Another important distinction to recognize is if the transformation is local (on one node eg) or not. These are referred to as narrow or wide transformations. \n",
    "\n",
    "** Narrow transformations ** (no transfer over network)\n",
    "map \n",
    "flatMap\n",
    "filter\n",
    "colalesce (generally local)\n",
    "\n",
    "** Wide transformations ** (can be expensive, involve shuffles)\n",
    "groupByKey\n",
    "repartition\n",
    "distinct\n",
    "subtract\n",
    "intersection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB \n",
    "\n",
    "    reduceByKey() and foldByKey() \n",
    "          \n",
    "will automatically perform combining _locally_ on each machine before computing global totals for each key.  \n",
    "        \n",
    "    combineByKey() \n",
    "  \n",
    "allows to customize combining behavior, use this instead of groupByKey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### two pair RDD  Transformations\n",
    "* subtractByKey\n",
    "* join\n",
    "* rightOuterJoin\n",
    "* leftOuterJoin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(spark_home +\"/README.md\")\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonlines = lines.filter(lambda line: \"Python\" in line)\n",
    "pythonlines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs = lines.map(lambda x: (x.split(\" \")[0], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pairs.filter(lambda keyValue: len(keyValue[1]) < 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(spark_home +\"/README.md\")\n",
    "words = rdd.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 67), ('guide,', 1), ('APIs', 1), ('name', 1), ('It', 2)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "result.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**parallelize** Distribute a local Python collection to form a partitioned\n",
    "RDD. Using xrange is recommended if the input represents a range for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[23] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"a\", 3), (\"b\", 4), (\"a\", 1)]\n",
    "sc.parallelize(data).reduceByKey(lambda x, y: x + y)      # Default parallelism\n",
    "sc.parallelize(data).reduceByKey(lambda x, y: x + y, 10)  # Custom parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rdd.sortByKey(ascending=True, numPartitions=None, keyfunc = lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 6), (3, 4)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 2 at the end means we split the list into 2 partitions\n",
    "rdd = sc.parallelize([(1, 2), (3, 6), (3, 4)],3)\n",
    "rdd.groupByKey()\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 2)], [(3, 6)], [(3, 4)]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see partitions use glom, useful also for debugging\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (3, 6)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)],2)\n",
    "rdd.reduceByKey(lambda x, y: x + y)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 3]\n",
      "[2, 4, 6]\n",
      "[(1, 2), (3, 4), (3, 6)]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.keys().collect())\n",
    "print(rdd.values().collect())\n",
    "print(rdd.sortByKey().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap() vs map()\n",
    "\n",
    "    map:  produce one to one input and output. note this is a completely **local** (narrow in spark-speak) operation. \n",
    "\n",
    "    flatMap:  produce multiple output elements for each input element, different sizes of partitions may be involved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'line',\n",
       " 'hello',\n",
       " 'second',\n",
       " 'line',\n",
       " 'third',\n",
       " 'guy']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize([\"this is the first line\", \"hello second line\", \"third guy\"])\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# can also use functions..set up word count key value pairs\n",
    "def split_words(line):\n",
    "    return line.split()\n",
    "\n",
    "def create_pair(word):\n",
    "    return (word,1)\n",
    "\n",
    "#equivalent to reduce.(lambda x,y: x+y)\n",
    "def sum_counts(a,b):\n",
    "    return a + b\n",
    "\n",
    "def starts_with_vowel(pair):\n",
    "    vowels = [\"a\",\"e\",\"i\",\"o\",\"u\"]\n",
    "    word = pair[0]\n",
    "    first_letter = word[0]\n",
    "    return first_letter.lower() in vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1), ('Apache', 1), ('Spark', 1), ('Spark', 1), ('is', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_rdd = text_file.flatMap(split_words).map(create_pair)\n",
    "pairs_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guide,', 1), ('APIs', 1), ('optimized', 1), ('name', 1), ('Scala,', 1)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce example\n",
    "wordcounts_rdd = pairs_rdd.reduceByKey(sum_counts)\n",
    "wordcounts_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('APIs', 1), ('Once', 1), ('only', 1), ('overview', 1), ('examples', 2)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter\n",
    "vs = pairs_rdd.filter(starts_with_vowel).reduceByKey(sum_counts)\n",
    "vs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guide,', <pyspark.resultiterable.ResultIterable at 0x7f28f4158d30>),\n",
       " ('APIs', <pyspark.resultiterable.ResultIterable at 0x7f28f4158cc0>),\n",
       " ('optimized', <pyspark.resultiterable.ResultIterable at 0x7f28f41685f8>),\n",
       " ('name', <pyspark.resultiterable.ResultIterable at 0x7f28f4168438>),\n",
       " ('Scala,', <pyspark.resultiterable.ResultIterable at 0x7f28f41686a0>)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group by key example\n",
    "pairs_rdd.groupByKey().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample with replacement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample without replacement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coalesce example\n",
    "sc.parallelize(range(10),4).coalesce(2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RDD Actions\n",
    "\n",
    "**actions** in spark-speak, are computations on RDDs. Action return non-RDD objects and values. Actually doing something with the data here, typically reside at the end of data analysis pipelines,\n",
    "eg, reduce. More examples:\n",
    "\n",
    "* take(),\n",
    "* top(),\n",
    "* first(), \n",
    "* count()\n",
    "* reduce()\n",
    "* aggregate()\n",
    "* fold()\n",
    "* collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 1, 3: 2})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)],2)\n",
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numeric RDD operations\n",
    "* count()\n",
    "* mean()\n",
    "* sum()\n",
    "* max()\n",
    "* min()\n",
    "* variance()\n",
    "* sampleVariance()\n",
    "* stdev()\n",
    "* sampleStdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Spark\n",
    "\n",
    "**joins**\n",
    "\n",
    "** broadcast variables** may be useful for large configuration, lookup tables\n",
    "\n",
    "**accumulators**  accumulate a variable across the cluster, concurrently write into a variable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_accum(x):\n",
    "    accum.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the famous Gauss summation 1 to 100\n",
    "accum = sc.accumulator(0)\n",
    "sc.parallelize(range(1,101)).foreach(test_accum)\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "here we will illustrate some HiveQL work on twitter stream data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "hiveCtx = HiveContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/matej/develop/mkm_notebooks/data/twitter'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get data from twitter, use api to download tweets.\n",
    "# python twitterstream.py > output.json\n",
    "import os\n",
    "os.chdir('/home/matej/develop/mkm_notebooks/data/twitter')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r--. 1 matej matej 188M Dec  8 19:54 output.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lah output.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='didi', text='RT @daiIygopro: Retweet if you want to travel the world üåéüåçüåè'),\n",
       " Row(name='‚ú®vega‚ú®‚Ñ¢', text='RT @vahlokmusic: Better'),\n",
       " Row(name='c4deb1t4', text='https://t.co/fAPWkVQdsD'),\n",
       " Row(name='HAL@12/11Kira„Ç´„É≥2.0Ë¶≥Ë¶ß', text='RT @Camus_SH: „Åä‰ªò„ÅçÂêà„ÅÑÈ†Ç„Åç„ÄÅ„ÅÇ„Çä„Åå„Å®„ÅÜ„Åî„Åñ„ÅÑ„Åæ„Åó„Åü„ÄÇ„Åæ„ÅüÊôÇÈñì„Åå„Åø„Å§„Åë„Å¶Á≠î„Åà„Å¶„ÅÑ„Åì„ÅÜ„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ'),\n",
       " Row(name='dan', text='Amber is the color of my energy‚ú®')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = hiveCtx.read.json('output.json')\n",
    "tweets.registerTempTable(\"tweets\")\n",
    "results = hiveCtx.sql(\"SELECT user.name, text FROM tweets\")\n",
    "results.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=53151)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get number of tweets\n",
    "r = hiveCtx.sql(\"SELECT count(*) FROM tweets\")\n",
    "r.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that spark comes with beeline, if compiled properly\n",
    "\n",
    "    ./sbt/sbt -Phive-thirftserver clean assembly/assembly\n",
    "\n",
    "activate via:\n",
    "\n",
    "    spark$ ./bin/beeline -u jdbc:hive2://\n",
    "    \n",
    "use standard beeline hive interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Spark:  MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(1000, {389: 1.0, 391: 1.0, 606: 1.0, 833: 1.0, 874: 1.0})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"hello there world of spark\"\n",
    "words = sentence.split()\n",
    "tf = HashingTF(1000)\n",
    "tf.transform(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note see file mkm_notebooks/license.txt for license of this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
