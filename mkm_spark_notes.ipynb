{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Tutorial\n",
    "\n",
    "Author: Matthew K. MacLeod\n",
    "\n",
    "\n",
    "### Tutorial goals:\n",
    "\n",
    "* background\n",
    "* configuration\n",
    "* introduction\n",
    "* HQL\n",
    "* machine learning\n",
    "* streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Spark is an open source project created in 2009 by the UC Berkeley RAD lab.\n",
    "\n",
    "Spark is a distributed framework which uses the MapReduce paradigm in memory. \n",
    "\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.html\n",
    "\n",
    "\n",
    "\n",
    "Allows for\n",
    "* interactive queries (Spark SQL and Hive)\n",
    "* stream processing\n",
    "* data analytics (MLlib)\n",
    "* graph processing (GraphX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "download and install spark,\n",
    "    \n",
    "    cd spark-1.5.2\n",
    "    \n",
    "    ./sbt/sbt -Phive assembly\n",
    "\n",
    "set environmental variables:\n",
    "\n",
    "    export SPARK_HOME=\"$HOME/programs/spark/spark-1.5.2\"\n",
    "    \n",
    "    export PYSPARK_SUBMIT_ARGS=\"--master local[4] pyspark-shell\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matej/programs/spark/spark-1.5.2\r\n"
     ]
    }
   ],
   "source": [
    "# double check env \n",
    "!echo $SPARK_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will load configuration in notebook..this way don't need to configure profile (ipython has issues)\n",
    "\n",
    "    ipython notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.5.2\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.4.3 (default, Oct 19 2015 21:52:17)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "# spark configuration\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "\n",
    "filename = os.path.join(spark_home, 'python/pyspark/shell.py')\n",
    "exec(compile(open(filename, \"rb\").read(), filename, 'exec'))\n",
    "\n",
    "spark_release_file = spark_home + \"/RELEASE\"\n",
    "\n",
    "if os.path.exists(spark_release_file) and \"Spark 1.5\" in open(spark_release_file).read():\n",
    "    pyspark_submit_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"\")\n",
    "    if not \"pyspark-shell\" in pyspark_submit_args: \n",
    "        pyspark_submit_args += \" pyspark-shell\"\n",
    "        os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will run spark interactively in jupyter, \n",
    "\n",
    "normally a pyspark script can be run\n",
    "\n",
    "    bin/spark-submit ps.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('guide,', 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_home = os.environ.get('SPARK_HOME')\n",
    "\n",
    "text_file = sc.textFile(spark_home + \"/README.md\")\n",
    "\n",
    "word_counts = text_file \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "word_counts.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**collect** Return a list that contains all of the elements in this RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RDDs\n",
    "\n",
    "An RDD (_resilient distributed dataset_) is an immutable distributed collection of objects. Since they are partitioned they can be computed on different nodes in cluster.\n",
    "\n",
    "**transformations** in spark-speak, operate on RDDs and return new RDDs. These are evaluated lazily.  map() and filter() are transformations.\n",
    "\n",
    "### one pair RDD  Transformations\n",
    "\n",
    "* reduceByKey\n",
    "* groupByKey\n",
    "* combineByKey\n",
    "* mapValues\n",
    "* flatMapValues\n",
    "* keys()\n",
    "* values()\n",
    "* sortByKey\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB \n",
    "\n",
    "    reduceByKey() and foldByKey() \n",
    "          \n",
    "will automatically perform combining locally on each machine before computing global totals for each key.  \n",
    "        \n",
    "    combineByKey() \n",
    "  \n",
    "allows to customize combining behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### two pair RDD  Transformations\n",
    "* subtractByKey\n",
    "* join\n",
    "* rightOuterJoin\n",
    "* leftOuterJoin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(spark_home +\"/README.md\")\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonlines = lines.filter(lambda line: \"Python\" in line)\n",
    "pythonlines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs = lines.map(lambda x: (x.split(\" \")[0], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pairs.filter(lambda keyValue: len(keyValue[1]) < 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(spark_home +\"/README.md\")\n",
    "words = rdd.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**parallelize** Distribute a local Python collection to form an RDD. Using xrange is recommended if the input represents a range for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [(\"a\", 3), (\"b\", 4), (\"a\", 1)]\n",
    "sc.parallelize(data).reduceByKey(lambda x, y: x + y)      # Default parallelism\n",
    "sc.parallelize(data).reduceByKey(lambda x, y: x + y, 10)  # Custom parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rdd.sortByKey(ascending=True, numPartitions=None, keyfunc = lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1, 2), (3, 6), (3, 4)],2)\n",
    "rdd.groupByKey()\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)],2)\n",
    "rdd.reduceByKey(lambda x, y: x + y)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(rdd.keys().collect())\n",
    "print(rdd.values().collect())\n",
    "print(rdd.sortByKey().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap() vs map()\n",
    "\n",
    "    map:  produce one to one input and output\n",
    "\n",
    "    flatMap:  produce multiple output elements for each input element, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'line',\n",
       " 'hello',\n",
       " 'second',\n",
       " 'line',\n",
       " 'third',\n",
       " 'guy']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize([\"this is the first line\", \"hello second line\", \"third guy\"])\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample with replacement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample without replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RDD Actions\n",
    "\n",
    "**actions** in spark-speak, are computations on RDDs. Actually doing someting with the data here. eg,\n",
    "\n",
    "* take(), \n",
    "* first(), \n",
    "* count()\n",
    "* reduce()\n",
    "* aggregate()\n",
    "* fold()\n",
    "* collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)],2)\n",
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numeric RDD operations\n",
    "* count()\n",
    "* mean()\n",
    "* sum()\n",
    "* max()\n",
    "* min()\n",
    "* variance()\n",
    "* sampleVariance()\n",
    "* stdev()\n",
    "* sampleStdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning in Spark:  MLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming in Spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
