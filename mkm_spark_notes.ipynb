{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Tutorial\n",
    "\n",
    "Author: Matthew K. MacLeod\n",
    "\n",
    "\n",
    "### Tutorial goals:\n",
    "\n",
    "* background\n",
    "* configuration\n",
    "* introduction\n",
    "  * transformations\n",
    "  * joins\n",
    "  * accumulators\n",
    "  * dataframes\n",
    "* HQL\n",
    "* machine learning\n",
    "* streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Spark is an open source project created in 2009 by the UC Berkeley RAD lab.\n",
    "\n",
    "Spark is a distributed framework which uses the MapReduce paradigm in memory and much more.. \n",
    "\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.html\n",
    "\n",
    "Allows for\n",
    "* interactive queries (Spark SQL and Hive)\n",
    "* stream processing\n",
    "* data analytics (MLlib)\n",
    "* graph processing (GraphX)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directed Acyclic Graph Scheduler\n",
    "\n",
    "Spark uses a DAG scheduler in order figure out how to execute the data analysis pipeline.\n",
    "\n",
    "DAGs help to make a dependency flow for the transformations..allows \n",
    "lineage and for recovery of lost data partitions.\n",
    "\n",
    "RDDs (spark objects, more on these below) are nodes and directed edges are are the transformations to create a execution graph.\n",
    "\n",
    "This DAG scheduler also allows for parallelization optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching\n",
    "\n",
    "Another important aspect to understanding Spark is caching. Since in- memory operations set Spark apart and is important to understand.\n",
    "\n",
    "in general \n",
    "    \n",
    "   * 10x to 100x speed ups typical\n",
    "    \n",
    "   * caching is gradual\n",
    "    \n",
    "   * fault tolerant (like rest of Spark)\n",
    "    \n",
    "   * want to cache the cleaned data set into memory\n",
    "    \n",
    "   * heavy calculations might want to use both memory and disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "python prereqs:\n",
    "\n",
    "    pip install py4j\n",
    "\n",
    "download and install spark with hive,\n",
    "    \n",
    "    cd spark-1.6.0\n",
    "    \n",
    "    ./build/sbt -Phive assembly\n",
    "    \n",
    "I do this to build with beeline as well:\n",
    "\n",
    "     ./build/sbt -Phive -Phive-thirftserver clean assembly/assembly\n",
    "\n",
    "set environmental variables:\n",
    "\n",
    "    export SPARK_HOME=\"$HOME/programs/spark/spark-1.6.0\"\n",
    "    \n",
    "    export PYSPARK_SUBMIT_ARGS=\"--master local[4] pyspark-shell\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matej/programs/spark/spark-1.6.0\r\n"
     ]
    }
   ],
   "source": [
    "# double check env \n",
    "!echo $SPARK_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will load configuration in notebook..this way don't need to configure profile\n",
    "\n",
    "    ipython notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.4.3 (default, Oct 19 2015 21:52:17)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "# spark configuration\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "\n",
    "filename = os.path.join(spark_home, 'python/pyspark/shell.py')\n",
    "exec(compile(open(filename, \"rb\").read(), filename, 'exec'))\n",
    "\n",
    "spark_release_file = spark_home + \"/RELEASE\"\n",
    "\n",
    "if os.path.exists(spark_release_file) and \"Spark 1.6\" in open(spark_release_file).read():\n",
    "    pyspark_submit_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"\")\n",
    "    if not \"pyspark-shell\" in pyspark_submit_args: \n",
    "        pyspark_submit_args += \" pyspark-shell\"\n",
    "        os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### small note on setup,\n",
    "\n",
    "it may have been quicker to simply use:\n",
    "\n",
    "\n",
    "     IPYTHON_OPTS=\"notebook --pylab inline\" ./bin/pyspark\n",
    "    \n",
    "to start the jupyter notebook, but I want to run with my own env."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark\n",
    "\n",
    "  * starting spark, intialization\n",
    "  * RDD transformations\n",
    "  * RDD actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will run spark interactively in jupyter, \n",
    "\n",
    "normally a pyspark script can be run\n",
    "\n",
    "    bin/spark-submit ps.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guide,', 1), ('APIs', 1), ('optimized', 1), ('name', 1), ('Scala,', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the standard spark 'hello world' example:\n",
    "spark_home = os.environ.get('SPARK_HOME')\n",
    "\n",
    "text_file = sc.textFile(spark_home + \"/README.md\")\n",
    "\n",
    "word_counts = text_file \\\n",
    "    .flatMap(lambda line: line.split()) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "word_counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**collect** Return a list that contains all of the elements in this RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RDDs\n",
    "\n",
    "An RDD (_resilient distributed dataset_) is an **immutable** distributed data container... like a collection of objects. These have distributed partitions across nodes and clusters.\n",
    "\n",
    "Resilency is realized by tracking partitions and re-running partition history.\n",
    "\n",
    "RDDs can be created from many sources, including local text, Amazon S3, JSON, HDFS, HBase, Cassandra and other sources.\n",
    "\n",
    "RDD operations consist of transformations and actions.\n",
    "\n",
    "**transformations** in spark-speak, operate on RDDs and *return new RDDs*. These are evaluated **lazily**.  eg map() and filter() are transformations.\n",
    "\n",
    "Due to the immutablity of the RDDs, functional style of transforming the data is essential.\n",
    "\n",
    "\n",
    "### one pair RDD  Transformations\n",
    "\n",
    "* map\n",
    "* filter\n",
    "* reduceByKey\n",
    "* groupByKey\n",
    "* combineByKey\n",
    "* mapValues\n",
    "* flatMapValues\n",
    "* keys\n",
    "* values\n",
    "* sortBy\n",
    "* sortByKey\n",
    "* sample(withReplacement, fraction ,seed)\n",
    "* coalesce(numPartitions)    to reduce number of partitions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locality\n",
    "\n",
    "Another important distinction to recognize is if the transformation is local (on one node eg) or not. These are referred to as narrow or wide transformations. \n",
    "\n",
    "** Narrow transformations ** (no transfer over network)\n",
    "* map \n",
    "* flatMap\n",
    "* filter\n",
    "* colalesce (generally local)\n",
    "\n",
    "** Wide transformations ** (can be expensive, involve shuffles)\n",
    "* groupByKey\n",
    "* repartition\n",
    "* distinct\n",
    "* subtract\n",
    "* intersection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB \n",
    "\n",
    "    reduceByKey() and foldByKey() \n",
    "          \n",
    "will automatically perform combining _locally_ on each machine before computing global totals for each key.  \n",
    "        \n",
    "    combineByKey() \n",
    "  \n",
    "allows to customize combining behavior, use this instead of groupByKey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### two pair RDD  Transformations\n",
    "* subtractByKey\n",
    "* join\n",
    "* rightOuterJoin\n",
    "* leftOuterJoin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.textFile(spark_home +\"/README.md\")\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'high-level APIs in Scala, Java, Python, and R, and an optimized engine that'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonlines = lines.filter(lambda line: \"Python\" in line)\n",
    "pythonlines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pairs = lines.map(lambda x: (x.split(\" \")[0], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = pairs.filter(lambda keyValue: len(keyValue[1]) < 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile(spark_home +\"/README.md\")\n",
    "words = rdd.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('', 67), ('guide,', 1), ('APIs', 1), ('name', 1), ('It', 2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "result.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**parallelize** Distribute a local Python collection to form a partitioned\n",
    "RDD. Using xrange is recommended if the input represents a range for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[28] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [(\"a\", 3), (\"b\", 4), (\"a\", 1)]\n",
    "sc.parallelize(data).reduceByKey(lambda x, y: x + y)      # Default parallelism\n",
    "sc.parallelize(data).reduceByKey(lambda x, y: x + y, 10)  # Custom parallelism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rdd.sortByKey(ascending=True, numPartitions=None, keyfunc = lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 6), (3, 4)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the 2 at the end means we split the list into 2 partitions\n",
    "rdd = sc.parallelize([(1, 2), (3, 6), (3, 4)],3)\n",
    "rdd.groupByKey()\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(1, 2)], [(3, 6)], [(3, 4)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to see partitions use glom, useful also for debugging\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (3, 6)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)],2)\n",
    "rdd.reduceByKey(lambda x, y: x + y)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 3]\n",
      "[2, 4, 6]\n",
      "[(1, 2), (3, 4), (3, 6)]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.keys().collect())\n",
    "print(rdd.values().collect())\n",
    "print(rdd.sortByKey().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap() vs map()\n",
    "\n",
    "    map: produce one to one input and output. note this is a completely  **local** (narrow in spark-speak) operation. \n",
    "\n",
    "    flatMap: produce multiple output elements for each input element, different sizes of partitions may be involved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'the',\n",
       " 'first',\n",
       " 'line',\n",
       " 'hello',\n",
       " 'second',\n",
       " 'line',\n",
       " 'third',\n",
       " 'guy']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = sc.parallelize([\"this is the first line\", \"hello second line\", \"third guy\"])\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# can also use functions..set up word count key value pairs\n",
    "def split_words(line):\n",
    "    return line.split()\n",
    "\n",
    "def create_pair(word):\n",
    "    return (word,1)\n",
    "\n",
    "#equivalent to reduce.(lambda x,y: x+y)\n",
    "def sum_counts(a,b):\n",
    "    return a + b\n",
    "\n",
    "def starts_with_vowel(pair):\n",
    "    vowels = [\"a\",\"e\",\"i\",\"o\",\"u\"]\n",
    "    word = pair[0]\n",
    "    first_letter = word[0]\n",
    "    return first_letter.lower() in vowels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#', 1), ('Apache', 1), ('Spark', 1), ('Spark', 1), ('is', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_rdd = text_file.flatMap(split_words).map(create_pair)\n",
    "pairs_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guide,', 1), ('APIs', 1), ('optimized', 1), ('name', 1), ('Scala,', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce example\n",
    "wordcounts_rdd = pairs_rdd.reduceByKey(sum_counts)\n",
    "wordcounts_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('APIs', 1), ('Once', 1), ('only', 1), ('overview', 1), ('examples', 2)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter\n",
    "vs = pairs_rdd.filter(starts_with_vowel).reduceByKey(sum_counts)\n",
    "vs.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('guide,', <pyspark.resultiterable.ResultIterable at 0x7fb2190fcbe0>),\n",
       " ('APIs', <pyspark.resultiterable.ResultIterable at 0x7fb2190fcda0>),\n",
       " ('optimized', <pyspark.resultiterable.ResultIterable at 0x7fb2190fce80>),\n",
       " ('name', <pyspark.resultiterable.ResultIterable at 0x7fb2190fce10>),\n",
       " ('Scala,', <pyspark.resultiterable.ResultIterable at 0x7fb2190fcf60>)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group by key example\n",
    "pairs_rdd.groupByKey().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample with replacement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample without replacement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coalesce example\n",
    "sc.parallelize(range(10),4).coalesce(2).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  RDD Actions\n",
    "\n",
    "**actions** in spark-speak, are computations on RDDs. Action return non-RDD objects and values. Actually doing something with the data here, typically reside at the end of data analysis pipelines,\n",
    "eg, reduce. More examples:\n",
    "\n",
    "* take(),\n",
    "* top(),\n",
    "* first(), \n",
    "* count()\n",
    "* reduce()\n",
    "* aggregate()\n",
    "* fold()\n",
    "* collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 1, 3: 2})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(1, 2), (3, 4), (3, 6)],2)\n",
    "rdd.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numeric RDD operations\n",
    "* count()\n",
    "* mean()\n",
    "* sum()\n",
    "* max()\n",
    "* min()\n",
    "* variance()\n",
    "* sampleVariance()\n",
    "* stdev()\n",
    "* sampleStdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Spark\n",
    "\n",
    "**joins**  basic joins are simple in spark\n",
    "\n",
    "** broadcast variables** may be useful for large configuration, lookup tables\n",
    "\n",
    "**accumulators**  accumulate a variable across the cluster, concurrently write into a variable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Join example\n",
    "\n",
    "Here we will combine two data sets and find the total number of views on each station (channel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hourly_Sports,DEF',\n",
       " 'Baked_News,BAT',\n",
       " 'PostModern_Talking,XYZ',\n",
       " 'Loud_News,CNO',\n",
       " 'Almost_Show,ABC',\n",
       " 'Hot_Talking,DEF',\n",
       " 'Dumb_Show,BAT',\n",
       " 'Surreal_Show,XYZ',\n",
       " 'Cold_Talking,CNO',\n",
       " 'Hourly_Cooking,ABC']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels = sc.textFile(\"./data/channels.txt\")\n",
    "channels.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hourly_Sports', 'DEF'],\n",
       " ['Baked_News', 'BAT'],\n",
       " ['PostModern_Talking', 'XYZ'],\n",
       " ['Loud_News', 'CNO'],\n",
       " ['Almost_Show', 'ABC']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_channel = channels.map(lambda x: x.split(\",\"))\n",
    "show_channel.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hourly_Sports,21',\n",
       " 'PostModern_Show,38',\n",
       " 'Surreal_News,73',\n",
       " 'Dumb_Cooking,144',\n",
       " 'Cold_Talking,287',\n",
       " 'Almost_Talking,574',\n",
       " 'Loud_News,113',\n",
       " 'Hot_Talking,228',\n",
       " 'Baked_Games,459',\n",
       " 'Hourly_Talking,922']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "views = sc.textFile(\"./data/views.txt\")\n",
    "views.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hourly_Sports', '21'],\n",
       " ['PostModern_Show', '38'],\n",
       " ['Surreal_News', '73'],\n",
       " ['Dumb_Cooking', '144'],\n",
       " ['Cold_Talking', '287']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_view = views.map(lambda x: x.split(\",\"))\n",
    "show_view.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hot_Show', ('631', 'ABC')),\n",
       " ('Hot_Show', ('631', 'XYZ')),\n",
       " ('Hot_Show', ('120', 'ABC')),\n",
       " ('Hot_Show', ('120', 'XYZ')),\n",
       " ('Hot_Show', ('477', 'ABC'))]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark will join on common key, here the show\n",
    "joined_data = show_view.join(show_channel)\n",
    "joined_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if not interested in the show, use this filter\n",
    "def extract_channel_views(show_views_channel):\n",
    "    ''' returns an array of views and channel '''\n",
    "    show, channel_views = show_views_channel[0], show_views_channel[1]\n",
    "    channel, views = channel_views[1], channel_views[0]\n",
    "    return (channel, views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ABC', '631'),\n",
       " ('XYZ', '631'),\n",
       " ('ABC', '120'),\n",
       " ('XYZ', '120'),\n",
       " ('ABC', '477')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel_views = joined_data.map(extract_channel_views)\n",
    "channel_views.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('631', 'ABC'),\n",
       " ('631', 'XYZ'),\n",
       " ('120', 'ABC'),\n",
       " ('120', 'XYZ'),\n",
       " ('477', 'ABC')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# side note: swaps are easy with map\n",
    "views_channel = channel_views.map(lambda x: (x[1], x[0]))\n",
    "views_channel.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ABC', 192745),\n",
       " ('XYZ', 224945),\n",
       " ('CNO', 233893),\n",
       " ('DEF', 252827),\n",
       " ('BAT', 195990)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect shows by key and sum views for each station \n",
    "channel_views.reduceByKey(lambda x,y: int(x) + int(y)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_accum(x): accum.add(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test the famous Gauss summation of 1 to 100\n",
    "accum = sc.accumulator(0)\n",
    "sc.parallelize(range(1,101)).foreach(test_accum)\n",
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrames\n",
    "\n",
    "These are great for real-world data science.\n",
    "\n",
    "Note that PySpark DataFrames are processed natively, so they are just as fast as Scala or Java equivalents!\n",
    "\n",
    "http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlCtx = sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_1: string, _2: bigint]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test --should return: DataFrame[_1: string, _2: bigint]\n",
    "sqlCtx.createDataFrame([(\"key\", 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why data frames are useful\n",
    "\n",
    "first illustrate the alternative without using dataframes\n",
    "\n",
    "some data:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Working_time\n",
    "\n",
    "https://stats.oecd.org/Index.aspx?DataSetCode=ANHRS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# small sampling from\n",
    "# Average usual weekly hours worked on the main job, 2014\n",
    "# tabular data, but not a data frame:\n",
    "hours = sc.parallelize([\n",
    "        [8, \"Poland\",39.9],\n",
    "        [1,\"Mexico\",44.7],\n",
    "        [34, \"France\",36.1],\n",
    "        [4, \"Greece\",38.8],\n",
    "        [17,\"United States\",38.6],\n",
    "        [38, \"Germany\",34.5]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_hours(row):\n",
    "    return row[2]\n",
    "\n",
    "def extract_country_weekly(row):\n",
    "    return (row[1], row[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38.766666666666666"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hours.map(extract_hours).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Poland', 39.9),\n",
       " ('Mexico', 44.7),\n",
       " ('France', 36.1),\n",
       " ('Greece', 38.8),\n",
       " ('United States', 38.6),\n",
       " ('Germany', 34.5)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_weekly_RDD = hours.map(extract_country_weekly)\n",
    "country_weekly_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mexico', 44.7),\n",
       " ('Poland', 39.9),\n",
       " ('Greece', 38.8),\n",
       " ('United States', 38.6),\n",
       " ('France', 36.1),\n",
       " ('Germany', 34.5)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the results \n",
    "country_weekly_RDD.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mexico', 44.7),\n",
       " ('Poland', 39.9),\n",
       " ('Greece', 38.8),\n",
       " ('United States', 38.6),\n",
       " ('France', 36.1),\n",
       " ('Germany', 34.5)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or this way\n",
    "country_weekly_RDD.sortBy(lambda x: -x[1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now do the same via dataframes\n",
    "\n",
    "should be a little less tedious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hours_schema = StructType([\n",
    "        StructField(\"rank\", LongType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"hours\", LongType(), True),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- rank: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- hours: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hours_df = sqlContext.createDataFrame(hours,hours_schema)\n",
    "hours_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+-----+\n",
      "|rank|         name|hours|\n",
      "+----+-------------+-----+\n",
      "|   8|       Poland| null|\n",
      "|   1|       Mexico| null|\n",
      "|  34|       France| null|\n",
      "|   4|       Greece| null|\n",
      "|  17|United States| null|\n",
      "|  38|      Germany| null|\n",
      "+----+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hours_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "For reference purposes,\n",
    "\n",
    "http://spark.apache.org/docs/latest/sql-programming-guide.html#sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that spark comes with beeline, if compiled properly\n",
    "\n",
    "    ./build/sbt -Phive-thirftserver clean assembly/assembly\n",
    "\n",
    "activate via:\n",
    "\n",
    "    spark$ ./bin/beeline -u jdbc:hive2://\n",
    "    \n",
    "use standard beeline hive interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/matej/programs/spark/spark-1.6.0\n"
     ]
    }
   ],
   "source": [
    "print(spark_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check example from docs\n",
    "df = sqlContext.read.json(spark_home + \"/examples/src/main/resources/people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['name'], df['age'] + 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Spark:  MLlib\n",
    "\n",
    "\n",
    "reference: \n",
    "\n",
    "http://spark.apache.org/docs/latest/mllib-guide.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = \"hello there world of spark\"\n",
    "words = sentence.split()\n",
    "tf = HashingTF(1000)\n",
    "tf.transform(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note see file mkm_notebooks/license.txt for license of this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
